{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fecd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# URL of the HTML page\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Step 1: Extract Text Data\n",
    "def extract_text_data():\n",
    "    headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2'])]\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "    list_items = [li.get_text(strip=True) for li in soup.find_all('li')]\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(\"Extract_Text_Data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Type\", \"Content\"])\n",
    "        for heading in headings:\n",
    "            writer.writerow([\"Heading\", heading])\n",
    "        for paragraph in paragraphs:\n",
    "            writer.writerow([\"Paragraph\", paragraph])\n",
    "        for item in list_items:\n",
    "            writer.writerow([\"List Item\", item])\n",
    "\n",
    "# Step 2: Extract Table Data\n",
    "def extract_table_data():\n",
    "    table = soup.find(\"table\")\n",
    "    rows = table.find_all(\"tr\")\n",
    "    headers = [header.get_text(strip=True) for header in rows[0].find_all(\"th\")]\n",
    "    data = [[cell.get_text(strip=True) for cell in row.find_all(\"td\")] for row in rows[1:]]\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(\"Extract_Table_Data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Step 3: Extract Product Information (Cards Section)\n",
    "def extract_product_information():\n",
    "    products = []\n",
    "    cards = soup.find_all(\"div\", class_=\"card\")\n",
    "    for card in cards:\n",
    "        product = {\n",
    "            \"Book Title\": card.find(\"h3\").get_text(strip=True),\n",
    "            \"Price\": card.find(\"span\", class_=\"price\").get_text(strip=True),\n",
    "            \"Stock Availability\": card.find(\"span\", class_=\"availability\").get_text(strip=True),\n",
    "            \"Button Text\": card.find(\"button\").get_text(strip=True),\n",
    "        }\n",
    "        products.append(product)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"Product_Information.json\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(products, file, indent=4)\n",
    "\n",
    "# Step 4: Extract Form Details\n",
    "def extract_form_details():\n",
    "    form = soup.find(\"form\")\n",
    "    inputs = form.find_all(\"input\")\n",
    "    form_data = []\n",
    "\n",
    "    for input_field in inputs:\n",
    "        field_info = {\n",
    "            \"Field Name\": input_field.get(\"name\"),\n",
    "            \"Input Type\": input_field.get(\"type\"),\n",
    "            \"Default Value\": input_field.get(\"value\", \"\"),\n",
    "        }\n",
    "        form_data.append(field_info)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"Form_Details.json\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(form_data, file, indent=4)\n",
    "\n",
    "# Step 5: Extract Links and Multimedia\n",
    "def extract_links_and_multimedia():\n",
    "    links = [{\"Text\": a.get_text(strip=True), \"Href\": a.get(\"href\")} for a in soup.find_all(\"a\")]\n",
    "    video_links = [{\"Video Link\": iframe.get(\"src\")} for iframe in soup.find_all(\"iframe\")]\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"Links_and_Multimedia.json\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump({\"Links\": links, \"Videos\": video_links}, file, indent=4)\n",
    "\n",
    "# Step 6: Scraping Challenge\n",
    "def extract_featured_products():\n",
    "    products = []\n",
    "    featured_products = soup.find_all(\"div\", class_=\"featured-product\")\n",
    "\n",
    "    for product in featured_products:\n",
    "        product_data = {\n",
    "            \"id\": product.get(\"data-id\"),\n",
    "            \"name\": product.find(\"span\", class_=\"name\").get_text(strip=True),\n",
    "            \"price\": product.find(\"span\", class_=\"price\").get_text(strip=True),\n",
    "            \"colors\": product.find(\"span\", class_=\"colors\").get_text(strip=True),\n",
    "        }\n",
    "        products.append(product_data)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"Featured_Products.json\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(products, file, indent=4)\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Web Scraping Task...\")\n",
    "    extract_text_data()\n",
    "    extract_table_data()\n",
    "    extract_product_information()\n",
    "    extract_form_details()\n",
    "    extract_links_and_multimedia()\n",
    "    extract_featured_products()\n",
    "    print(\"Web Scraping Task Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
